{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.util import Surv as skSurv\n",
    "from sksurv.metrics import concordance_index_ipcw, cumulative_dynamic_auc, brier_score, integrated_brier_score\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.utils import resample\n",
    "from utils.param_search import *\n",
    "from utils.output_results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c2691-401b-4b98-92e6-e3e186879bfb",
   "metadata": {},
   "source": [
    "The uncertainty measure used here is the MC-Dropout method.\n",
    "\n",
    "2 real datasets can be used : \n",
    "- the LungCancerExplorer dataset is a set composed of multiple sources of data\n",
    "- the METABRIC cohort.\n",
    "\n",
    "We apply this method to different types of neural network models :\n",
    "- CoxCC, CoxTime\n",
    "- DeepHit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5836d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', '-dt', default='Metabric', type=str) #LungCancerExplorer, Metabric\n",
    "parser.add_argument('--name', '-n',type=str, default=\"CoxCC\")#CoxTime, DeepHit, CoxCC \n",
    "parser.add_argument('--plot_mode', '-pm', default=False, action='store_true')\n",
    "parser.add_argument('--timepoints', '-tp',type=str, default=\"fixed\") #fixed, percentiles\n",
    "parser.add_argument('--uncertainty', '-u',type=str, default=\"MCDropout\") #Bootstrap, MCDropout, DeepEnsemble, VAE, BMask\n",
    "config = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124177c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_res = 'results/'+ config.dataset + \"/\"+config.uncertainty+\"/\"+ config.name+'/'\n",
    "os.makedirs(dir_res, exist_ok=True)\n",
    "\n",
    "dir_data = \"data/\" + config.dataset + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacf2b4",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63934c-8ef8-43ea-a184-3d489a6613fe",
   "metadata": {},
   "source": [
    "Missing data are previously handled using the MICE package in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11eda5-f873-4691-82d8-8b57bc038dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_data+\"DataImputed.csv\")\n",
    "df['id'] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57d7de-c426-478f-9025-d9c6e3cc4998",
   "metadata": {},
   "source": [
    "We list the clinical variables and the genes variables according to the dataset considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb57f3-d86a-4e75-9ca0-74926a50c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.dataset == \"Metabric\":\n",
    "    ClinVar =['age',\n",
    "                   'chemotherapy',\n",
    "                   'grade1',\n",
    "                   'grade2',\n",
    "                   'hormonotherapy',\n",
    "                   'N',\n",
    "                   'tumor_size']\n",
    "    ColsLeave = ['chemotherapy',\n",
    "                  'grade1',\n",
    "                  'grade2',\n",
    "                  'id',\n",
    "                  'hormonotherapy',\n",
    "                  'N',\n",
    "                  'status',\n",
    "                  'yy']\n",
    "elif config.dataset == \"LungCancerExplorer\":\n",
    "    ClinVar = ['Pat_Age',\n",
    "                    'Pat_Stage_II',\n",
    "                    'Pat_Stage_III',\n",
    "                    'Pat_Stage_IV',\n",
    "                    'Pat_Stage_I_or_II']\n",
    "    ColsLeave = ['id',\n",
    "                  'Pat_Stage_II',\n",
    "                  'Pat_Stage_III',\n",
    "                  'Pat_Stage_IV',\n",
    "                  'Pat_Stage_I_or_II',\n",
    "                  'status',\n",
    "                  'yy']\n",
    "GenesVar = df.drop(columns = ClinVar + ['id','status','yy'],axis=1).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58410d7a-636c-45ad-8dd0-915e1020b593",
   "metadata": {},
   "source": [
    "The 5 folds of the simple cross-validation are defined and saved beforehand in order to always have the same folds between the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baa45d-4e9c-4244-8646-580899c25bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_data+\"folds_1CV.json\") as f:\n",
    "    kfolds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e82b9-fdcc-4cae-9395-da0897c4dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[ClinVar+GenesVar+['yy','status','id']].loc[kfolds['train']] \n",
    "df_test = df[ClinVar+GenesVar+['yy','status','id']].loc[kfolds['test']] \n",
    "AllVar = ClinVar+GenesVar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49745246-7c8f-4694-8281-546ba2091e55",
   "metadata": {},
   "source": [
    "The continuous variable are standardized. The yy variable corresponds to the survival time, the status is the censoring indicator (a value of 0 corresponds to censoring) and the id variable is the id of the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de4ec7-6b67-4770-b5ff-e8f5492eff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColsStandardize = [col for col in AllVar if col not in ColsLeave] \n",
    "standardize = [([col], StandardScaler()) for col in ColsStandardize]\n",
    "leave = [(col, None) for col in ColsLeave]\n",
    "df_mapper = DataFrameMapper(standardize + leave, df_out=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1712c1-f9a1-42c6-92bb-2e6aabbf693e",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbd0db-eaff-48f9-9ccc-a5eab13548a5",
   "metadata": {},
   "source": [
    "A simple 5-folds cross validation is implemented using the training set to determine the hyperparameters of the neural network models. The optuna package is used to perform the hyperparmeter search. The hyperparameter that are searched are the following:\n",
    "\n",
    "| Hyperparameter | Values |\n",
    "|----------|--------------|\n",
    "| Activation function |  {tanh, relu}|\n",
    "| Batch size |  {8,16,32,64,128} |\n",
    "| Dropout rate |  [0.0,0.3] | \n",
    "|Layers | {1,2,3,4}|\n",
    "|Learning rate|[1e-3, 1e-2]|\n",
    "|Neurons|[4,128]|\n",
    "|Optimizer|{adam, adam_amsgrad, RMSProp, SGDWR}|\n",
    "|PÃ©nalisation L2|[0,0.1]|\n",
    "|$\\tau$ (MCDropout)|[0.025, 0.2]|\n",
    "|$\\alpha$ (DeepHit)|[0,1]|\n",
    "|$\\sigma$(DeepHit)|{0.1,0.25,0.5,1,2.5,5,10,100}|\n",
    "|Durations(DeepHit)|{10,50,100,200,400}|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc019e7-7a2f-4581-aa7e-38ee87753720",
   "metadata": {},
   "source": [
    "For MCDropout, dropout is applied both during training and test. Moreover a specific l2 weight regularisation term is computed and applied to the model. It corresponds to choosing a prior $\\tau$. \n",
    "\n",
    "Initially, we have : \n",
    "$$\\tau = \\frac{(1-p)l^{2}}{2N\\lambda}$$\n",
    "$l$ is defined as the prior lengthsacale, $\\lambda$ as the weight decay, $p$ is the dropout rate. $N$ is the length of the data. We choose $l$, $p$ and $\\tau$ by cross-validation. We reformulate the first equation and obtain $\\lambda$ as :\n",
    "$$\\lambda = \\frac{(1-p)l^{2}}{2N\\tau}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28e254-1a0a-4d42-88d8-74c0ac52283c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(study_name = config.name, \n",
    "                            storage = 'sqlite:///'+dir_res+config.name+ '.db',\n",
    "                            sampler=sampler, \n",
    "                            direction='minimize', \n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(lambda trial : objective_net(trial,\n",
    "                                            df_train,\n",
    "                                            df_mapper,\n",
    "                                            dir_res,\n",
    "                                            config), \n",
    "               n_trials=2)\n",
    "\n",
    "trial = study.best_trial\n",
    "outer_loop = pd.DataFrame([trial.params])\n",
    "outer_loop.to_csv(dir_res + 'best_param.csv', sep = ';', index = False, header = True)\n",
    "df_results = study.trials_dataframe()\n",
    "df_results.to_csv(dir_res + 'trials_dataframe.csv', sep = ';', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f8be4",
   "metadata": {},
   "source": [
    "# Uncertainty measure using MCDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be85e9-28a8-477d-977c-4db7be53c91d",
   "metadata": {},
   "source": [
    "The model is built using the hyperparameters taht were selected using the 5-folds cross-validation. M predictions per point on the test set are outputed by applying dropout M times randomly at test time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d9f75-5ee6-42c7-a452-e29a5cf466db",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loop = pd.read_csv(dir_res+'best_param.csv', sep = ';')\n",
    "config.acti_func = outer_loop['activation'][0]\n",
    "config.batch_size = outer_loop['batch_size'][0]\n",
    "config.layers = outer_loop['n_layers'][0]\n",
    "config.lr  = outer_loop['learning_rate'][0]\n",
    "config.neurons = outer_loop['neurons'][0]\n",
    "config.optim = outer_loop['optimizer'][0]\n",
    "\n",
    "if config.name==\"DeepHit\":\n",
    "    config.alpha = outer_loop['alpha'][0] \n",
    "    config.sigma = outer_loop['sigma'][0]\n",
    "    config.num_durations = outer_loop['num_durations'][0]\n",
    "    labtrans = DeepHitSingle.label_transform(config.num_durations)\n",
    "elif config.name==\"CoxTime\":\n",
    "    labtrans = CoxTime.label_transform()\n",
    "else:\n",
    "    labtrans=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7f4af-4670-42ed-8509-3750918ed6a2",
   "metadata": {},
   "source": [
    "We define the number of repetitions, M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2965979",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82baeb10-4e63-4a5d-9171-2a6091537d42",
   "metadata": {},
   "source": [
    "We sample 20\\% of the train set and define it as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec08506-2141-4d7d-9357-0aa9921a6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_train.sample(frac=0.2, random_state = 1234)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "df_train = df_mapper.fit_transform(df_train)\n",
    "df_val = df_mapper.transform(df_val).astype('float32')\n",
    "df_test = df_mapper.transform(df_test).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2d14f-562c-4b69-b905-4fa9703841a0",
   "metadata": {},
   "source": [
    "If the timepoints are defined as percentiles, the results are outputed at the percentiles of survival times of the data. If the timepoints are deifned as fixed, the results are outputed at specific times.\n",
    "\n",
    "For the breast cancer dataset, the fixed timepoints used are at 5 and 10 years. For the LungCancerExplorer, we output the measures at 2 and 5 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d18d6c-a4c3-4ff0-b5d6-fe75c00b82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.timepoints == \"percentiles\":\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(np.array(df_train['yy']), np.array(df_train['status']))\n",
    "    time_grid = np.array(kmf.percentile(np.linspace(0.9, 0.1, 9)).iloc[:,0])\n",
    "elif config.timepoints == \"fixed\":\n",
    "    if config.dataset == \"Metabric\":\n",
    "        time_grid = [2,5]\n",
    "    elif config.dataset == \"LungCancerExplorer\":\n",
    "        time_grid = [24,60]\n",
    "    else:\n",
    "        time_grid = [0.5,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58fb80-152e-496d-b5f7-47754feecb35",
   "metadata": {},
   "source": [
    "We define x and y for the train set, the validation set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168a0ce-5f85-475d-a891-481f34a1526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(df_train.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "x_val = np.array(df_val.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "x_test = np.array(df_test.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "y_train = (df_train['yy'].values, df_train['status'].values)\n",
    "y_val = (df_val['yy'].values, df_val['status'].values)\n",
    "y_test = (df_test['yy'].values, df_test['status'].values)\n",
    "    \n",
    "if labtrans !=\"\":\n",
    "    y_train = labtrans.fit_transform(*y_train)\n",
    "    y_val = labtrans.transform(*y_val)\n",
    "\n",
    "val = (x_val, y_val)\n",
    "in_features = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343f52b-c90b-416c-bc40-29ae34cd436e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_samples = x_train.shape[0]\n",
    "config.tau = outer_loop['tau'][0]\n",
    "lengthscale = 1e-2\n",
    "reg = lengthscale**2 * (1 - config.dr) / (2. * in_samples * config.tau)\n",
    "config.pen_l2 = reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935183c-eb1c-4328-a394-a3816cfe85af",
   "metadata": {},
   "source": [
    "We compute the Concordance Index (CAll), the Brier Score (BSAll) and the survival predictions for all the patients of the test set (PredAll) for each timepoint of the time grid previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d9f95-a8aa-44d6-b1f8-c9a5d9d5b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAll = pd.DataFrame()\n",
    "BSAll = pd.DataFrame()\n",
    "PredAll = []\n",
    "measures = pd.DataFrame()\n",
    "\n",
    "for j in range(M):\n",
    "    print(j)\n",
    "    model,callbacks = build_model_net(config,\n",
    "                                      in_features,\n",
    "                                      labtrans)\n",
    "    enable_dropout(model.net)\n",
    "    log = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    int(config.batch_size),\n",
    "                    epochs = 100, \n",
    "                    callbacks = callbacks,\n",
    "                    verbose = False,\n",
    "                    val_data = val)\n",
    "\n",
    "    df_log = log.to_pandas()[['train_loss', 'val_loss']]\n",
    "    if config.name in [\"CoxCC\",\"CoxTime\"]:\n",
    "        _ = model.compute_baseline_hazards()\n",
    "        surv = model.predict_surv_df(np.array(x_test))\n",
    "    elif config.name == \"DeepHit\":\n",
    "        surv = model.interpolate(10).predict_surv_df(x_test)\n",
    "\n",
    "    data_train = skSurv.from_arrays(event=df_train['status'], time=df_train['yy'])\n",
    "    data_test = skSurv.from_arrays(event=df_test['status'], time=df_test['yy'])\n",
    "    CAll[j] = [concordance_index_ipcw(data_train, data_test, np.array(-determine_surv_prob(surv,t)),t)[0] for t in time_grid]\n",
    "    BSAll[j] = [brier_score(data_train, data_test, np.array(-determine_surv_prob(surv,t)),t)[1][0] for t in time_grid]\n",
    "    Pred = np.asarray([determine_surv_prob(surv,t) for t in time_grid])\n",
    "    PredAll.append(Pred)\n",
    "    del model\n",
    "    del log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7707-ec69-49d2-80b3-8c1686d3e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures['C'] = CAll.mean(axis=1)\n",
    "measures['BS'] = BSAll.mean(axis=1)\n",
    "measures['Time'] = time_grid\n",
    "measures.to_csv(dir_res+'measures_'+config.name+'.csv', sep = ';', header = True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b4c7e-7b4d-4bea-8f1c-2e8d709b6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b138f95-431c-4093-9579-0c4e41ab0436",
   "metadata": {},
   "source": [
    "We output survival prediction intervals for a certain number of patients (n_pat) for each timepoint of the timegrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d217e-7cc0-4f96-8a86-1bfdbe693d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733db57-a76d-4445-b2d8-7520a2c8785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4893)\n",
    "patients_test = random.sample(list(df_test['id']),n_pat)\n",
    "ResPat = []\n",
    "for t in range (len(time_grid)):\n",
    "    ResTime = pd.DataFrame([PredAll[l][t] for l in range(M)]).T\n",
    "    for i in patients_test:\n",
    "        ResTime.index = df_test['id']\n",
    "        ic = output_ic(ResTime.loc[i,:],0.95)\n",
    "        values = df_test.loc[:,['status','yy','id']][df_test['id']==i]\n",
    "        ResPat.append(values.values.tolist()[0]+list(ic))\n",
    "ResPat = pd.DataFrame(ResPat)\n",
    "ResPat.columns = ['status','yy','id','IClow','ICmean','IChigh']\n",
    "ResPat[\"Time\"] = np.repeat(time_grid,n_pat)\n",
    "ResPat.to_csv(dir_res+\"surv_intervals.csv\", sep=';', header = True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4490514-2c42-4f68-a099-1061ac43ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResPat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
